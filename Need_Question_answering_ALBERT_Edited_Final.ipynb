{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **IE7500 NLP Final Project**\n",
        "# *SQuADv2.0 Answer Prediction Model using KNN and BERT models*\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "e49BmLCxQpsh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Importing Libraries**"
      ],
      "metadata": {
        "id": "6n6_KDPHCzfj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installing the transformers"
      ],
      "metadata": {
        "id": "l_I0RSrcq4XE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r3EVM8p56tpK",
        "outputId": "97364524-c619-4a58-efb1-48f956afc703"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (4.28.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.11.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installing other necessary libraries"
      ],
      "metadata": {
        "id": "zszLU2ufrBRf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "UvDcnyzhAwS2"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from transformers import AutoTokenizer\n",
        "import os, json\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question Answering using BERT Model**"
      ],
      "metadata": {
        "id": "AztbimcDWv0o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reading the input from JSON file and storing in list\n",
        "\n"
      ],
      "metadata": {
        "id": "NTFzOqsisHAh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_input(doc: str) -> tuple:    \n",
        "    path = os.path.join(os.getcwd(), doc)\n",
        "    with open(path, \"rb\") as json_file:\n",
        "        input_dictionary = json.load(json_file)\n",
        "    contexts= list() \n",
        "    questions=list() \n",
        "    answers =list()\n",
        "    for i in input_dictionary['data']:\n",
        "        for j in i['paragraphs']:\n",
        "            k = j['context']\n",
        "            for qa in j['qas']:\n",
        "                q = qa['question']\n",
        "                access = \"plausible_answers\" if \"plausible_answers\" in qa.keys() else 'answers'\n",
        "                for a in qa[access]:\n",
        "                    contexts.append(k)\n",
        "                    questions.append(q)\n",
        "                    answers.append(a)\n",
        "    \n",
        "    return contexts, questions, answers\n",
        "\n"
      ],
      "metadata": {
        "id": "383hyUnZrLOV"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reading test and train data"
      ],
      "metadata": {
        "id": "L4UGvg1Fu8Pg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_contexts, train_questions, train_answers = read_input('train-v2.0.json')\n",
        "valid_contexts, valid_questions, valid_answers = read_input('dev-v2.0.json')"
      ],
      "metadata": {
        "id": "Kr4ITGV3L20N"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Printing 5 random samples to check if data is uploaded and processed correctly"
      ],
      "metadata": {
        "id": "3fz6Ehhku_EL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ind = random.sample(range(0, len(train_answers)), 5)\n",
        "for index in ind:\n",
        "    print('Q: ',train_questions[index],'\\n')\n",
        "    print(\"Context:\\n\")\n",
        "    print(train_contexts[index])\n",
        "    print(f\"\\nAnswer:[{train_answers[index]}]\\n\")\n",
        "    print(\"*\" * 100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r_ysQvcKL4LY",
        "outputId": "bb5a5468-6a94-4941-cf0b-7ef3d54aa5e2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q:  For how many seasons was American Idol the most watched show in the US? \n",
            "\n",
            "Context:\n",
            "\n",
            "In 2001, Fuller, Cowell, and TV producer Simon Jones attempted to sell the Pop Idol format to the United States, but the idea was met with poor response from United States television networks. However, Rupert Murdoch, head of Fox's parent company, was persuaded to buy the show by his daughter Elisabeth, who was a fan of the British show. The show was renamed American Idol: The Search for a Superstar and debuted in the summer of 2002. Cowell was initially offered the job as showrunner but refused; Lythgoe then took over that position. Much to Cowell's surprise, it became one of the hit shows for the summer that year. The show, with the personal engagement of the viewers with the contestants through voting, and the presence of the acid-tongued Cowell as a judge, grew into a phenomenon. By 2004, it had become the most-watched show in the U.S., a position it then held on for seven consecutive seasons.\n",
            "\n",
            "Answer:[{'text': 'seven', 'answer_start': 884}]\n",
            "\n",
            "****************************************************************************************************\n",
            "Q:  Which doctor from Pakistan served until 2000? \n",
            "\n",
            "Context:\n",
            "\n",
            "Executive Directors and Under-Secretaries General of the UN\n",
            "2011–present Dr Babatunde Osotimehin (Nigeria)\n",
            "2000–2010 Ms Thoraya Ahmed Obaid (Saudi Arabia)\n",
            "1987–2000 Dr Nafis Sadik (Pakistan)\n",
            "1969–87 Mr Rafael M. Salas (Philippines)\n",
            "\n",
            "Answer:[{'text': 'Dr Nafis Sadik', 'answer_start': 165}]\n",
            "\n",
            "****************************************************************************************************\n",
            "Q:  What country has reduced its HIV mortality rate the most? \n",
            "\n",
            "Context:\n",
            "\n",
            "Tuberculosis is the second-most common cause of death from infectious disease (after those due to HIV/AIDS). The total number of tuberculosis cases has been decreasing since 2005, while new cases have decreased since 2002. China has achieved particularly dramatic progress, with about an 80% reduction in its TB mortality rate between 1990 and 2010. The number of new cases has declined by 17% between 2004–2014. Tuberculosis is more common in developing countries; about 80% of the population in many Asian and African countries test positive in tuberculin tests, while only 5–10% of the US population test positive. Hopes of totally controlling the disease have been dramatically dampened because of a number of factors, including the difficulty of developing an effective vaccine, the expensive and time-consuming diagnostic process, the necessity of many months of treatment, the increase in HIV-associated tuberculosis, and the emergence of drug-resistant cases in the 1980s.\n",
            "\n",
            "Answer:[{'text': 'China', 'answer_start': 223}]\n",
            "\n",
            "****************************************************************************************************\n",
            "Q:  What did the Democrats present in 2015 \n",
            "\n",
            "Context:\n",
            "\n",
            "On 23 April 2014, the U.S. Federal Communications Commission (FCC) was reported to be considering a new rule that will permit ISPs to offer content providers a faster track to send content, thus reversing their earlier net neutrality position. A possible solution to net neutrality concerns may be municipal broadband, according to Professor Susan Crawford, a legal and technology expert at Harvard Law School. On 15 May 2014, the FCC decided to consider two options regarding Internet services: first, permit fast and slow broadband lanes, thereby compromising net neutrality; and second, reclassify broadband as a telecommunication service, thereby preserving net neutrality. On 10 November 2014, President Barack Obama recommended that the FCC reclassify broadband Internet service as a telecommunications service in order to preserve net neutrality. On 16 January 2015, Republicans presented legislation, in the form of a U.S. Congress H.R. discussion draft bill, that makes concessions to net neutrality but prohibits the FCC from accomplishing the goal or enacting any further regulation affecting Internet service providers. On 31 January 2015, AP News reported that the FCC will present the notion of applying (\"with some caveats\") Title II (common carrier) of the Communications Act of 1934 to the internet in a vote expected on 26 February 2015. Adoption of this notion would reclassify internet service from one of information to one of the telecommunications and, according to Tom Wheeler, chairman of the FCC, ensure net neutrality. The FCC is expected to enforce net neutrality in its vote, according to the New York Times.\n",
            "\n",
            "Answer:[{'text': 'a U.S. Congress H.R. discussion draft bill', 'answer_start': 924}]\n",
            "\n",
            "****************************************************************************************************\n",
            "Q:  Before insulin, what was the life expectancy of diabetics? \n",
            "\n",
            "Context:\n",
            "\n",
            "A series of experiments performed from the late 1800s to the early 1900s revealed that diabetes is caused by the absence of a substance normally produced by the pancreas. In 1869, Oskar Minkowski and Joseph von Mering found that diabetes could be induced in dogs by surgical removal of the pancreas. In 1921, Canadian professor Frederick Banting and his student Charles Best repeated this study, and found that injections of pancreatic extract reversed the symptoms produced by pancreas removal. Soon, the extract was demonstrated to work in people, but development of insulin therapy as a routine medical procedure was delayed by difficulties in producing the material in sufficient quantity and with reproducible purity. The researchers sought assistance from industrial collaborators at Eli Lilly and Co. based on the company's experience with large scale purification of biological materials. Chemist George Walden of Eli Lilly and Company found that careful adjustment of the pH of the extract allowed a relatively pure grade of insulin to be produced. Under pressure from Toronto University and a potential patent challenge by academic scientists who had independently developed a similar purification method, an agreement was reached for non-exclusive production of insulin by multiple companies. Prior to the discovery and widespread availability of insulin therapy the life expectancy of diabetics was only a few months.\n",
            "\n",
            "Answer:[{'text': 'only a few months', 'answer_start': 1411}]\n",
            "\n",
            "****************************************************************************************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bert models require the end position of the answers, therefore finding the end point of for each answer"
      ],
      "metadata": {
        "id": "RNZ5B1mv0bJ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def end_point(answers: list, contexts: list) -> list:\n",
        "    _answers = answers.copy()\n",
        "    for answer, context in zip(_answers, contexts):\n",
        "        end_idx = answer['text']\n",
        "        start_idx = answer['answer_start']\n",
        "        answer['answer_end'] = start_idx + len(end_idx)\n",
        "    return _answers"
      ],
      "metadata": {
        "id": "rd4BySotL6vv"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_answers = end_point(train_answers, train_contexts)\n",
        "valid_answers = end_point(valid_answers, valid_contexts)"
      ],
      "metadata": {
        "id": "VGWhQ8nfL8ND"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we fine-tune the pre-trained model, we use ALBERT model"
      ],
      "metadata": {
        "id": "YdacdSwQ0nYQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('albert-base-v2', use_fast=True)"
      ],
      "metadata": {
        "id": "zs7l5sCAL9v2"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encoding the data to train the model"
      ],
      "metadata": {
        "id": "F4fonjez3pZV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_data(contexts: list, questions: list, answers: list) -> dict:\n",
        "    encodings = tokenizer(contexts, questions, truncation=True, padding=True, return_tensors=\"pt\")\n",
        "    start_pos, end_pos = list(), list()\n",
        "\n",
        "    for index in range(len(answers)):\n",
        "        start_value = encodings.char_to_token(index, answers[index]['answer_start'])\n",
        "        end_value   = encodings.char_to_token(index, answers[index]['answer_end'])\n",
        "        if start_value is None:\n",
        "            start_value = tokenizer.model_max_length\n",
        "\n",
        "        shift = 1\n",
        "        while end_value is None:\n",
        "            end_value = encodings.char_to_token(index, answers[index]['answer_end'] - shift)\n",
        "            shift += 1\n",
        "\n",
        "        start_pos.append(start_value)\n",
        "        end_pos.append(end_value)\n",
        "\n",
        "    encodings.update({\n",
        "        'start_positions': start_pos, 'end_positions': end_pos\n",
        "    })\n",
        "    return encodings"
      ],
      "metadata": {
        "id": "thiCA2RsL9ub"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use sample of data to train since training the full dataset will take lot of time and with the technological constraint, It is hard"
      ],
      "metadata": {
        "id": "4c6dXfBm3wIE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_encodings = encode_data(train_contexts[0:100], train_questions[0:100], train_answers[0:100])\n",
        "valid_encodings = encode_data(valid_contexts[0:500], valid_questions[0:500], valid_answers[0:500])"
      ],
      "metadata": {
        "id": "F0IiiIe2MA_b"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Deleting unwanted variables to prevent crashing"
      ],
      "metadata": {
        "id": "oBHauN784Alm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "del train_contexts, train_questions, train_answers\n",
        "del valid_contexts, valid_questions, valid_answers"
      ],
      "metadata": {
        "id": "1NrKtbK0MCTH"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "class SquadDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings: dict) -> None:\n",
        "        self.encodings = encodings\n",
        "\n",
        "    def __getitem__(self, index: int) -> dict:\n",
        "        return {key: torch.tensor(val[index]) for key, val in self.encodings.items()}\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.encodings['input_ids'])"
      ],
      "metadata": {
        "id": "-gbsVrlMMDqk"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = SquadDataset(train_encodings)\n",
        "valid_ds = SquadDataset(valid_encodings)"
      ],
      "metadata": {
        "id": "zdCY-WJiMFwK"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForQuestionAnswering\n",
        "model = AutoModelForQuestionAnswering.from_pretrained('albert-base-v2')\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "model.to(device)\n",
        "model.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CqVgNioxMHma",
        "outputId": "0c533e65-750d-4592-a0bf-b16e576f65c2"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at albert-base-v2 were not used when initializing AlbertForQuestionAnswering: ['predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.dense.weight']\n",
            "- This IS expected if you are initializing AlbertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing AlbertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of AlbertForQuestionAnswering were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AlbertForQuestionAnswering(\n",
              "  (albert): AlbertModel(\n",
              "    (embeddings): AlbertEmbeddings(\n",
              "      (word_embeddings): Embedding(30000, 128, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 128)\n",
              "      (token_type_embeddings): Embedding(2, 128)\n",
              "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0, inplace=False)\n",
              "    )\n",
              "    (encoder): AlbertTransformer(\n",
              "      (embedding_hidden_mapping_in): Linear(in_features=128, out_features=768, bias=True)\n",
              "      (albert_layer_groups): ModuleList(\n",
              "        (0): AlbertLayerGroup(\n",
              "          (albert_layers): ModuleList(\n",
              "            (0): AlbertLayer(\n",
              "              (full_layer_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (attention): AlbertAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (attention_dropout): Dropout(p=0, inplace=False)\n",
              "                (output_dropout): Dropout(p=0, inplace=False)\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              )\n",
              "              (ffn): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (ffn_output): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (activation): NewGELUActivation()\n",
              "              (dropout): Dropout(p=0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "This cell is adopted from `https://github.com/michaelrzhang/lookahead/blob/master/lookahead_pytorch.py`, which is the\n",
        "source code of `Lookahead Optimizer: k steps forward, 1 step back` paper (https://arxiv.org/abs/1907.08610).\n",
        "\"\"\"\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "import torch\n",
        "from torch.optim.optimizer import Optimizer\n",
        "\n",
        "\n",
        "class LookaheadWrapper(Optimizer):\n",
        "    r\"\"\"PyTorch implementation of the lookahead wrapper.\n",
        "    Lookahead Optimizer: https://arxiv.org/abs/1907.08610\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, optimizer, la_steps=5, la_alpha=0.8, pullback_momentum=\"none\"):\n",
        "        \"\"\"optimizer: inner optimizer\n",
        "        la_steps (int): number of lookahead steps\n",
        "        la_alpha (float): linear interpolation factor. 1.0 recovers the inner optimizer.\n",
        "        pullback_momentum (str): change to inner optimizer momentum on interpolation update\n",
        "        \"\"\"\n",
        "        self.optimizer = optimizer\n",
        "        self._la_step = 0  # counter for inner optimizer\n",
        "        self.la_alpha = la_alpha\n",
        "        self._total_la_steps = la_steps\n",
        "        pullback_momentum = pullback_momentum.lower()\n",
        "        assert pullback_momentum in [\"reset\", \"pullback\", \"none\"]\n",
        "        self.pullback_momentum = pullback_momentum\n",
        "\n",
        "        self.state = defaultdict(dict)\n",
        "\n",
        "        # Cache the current optimizer parameters\n",
        "        for group in optimizer.param_groups:\n",
        "            for param in group['params']:\n",
        "                param_state = self.state[param]\n",
        "                param_state['cached_params'] = torch.zeros_like(param.data)\n",
        "                param_state['cached_params'].copy_(param.data)\n",
        "                if self.pullback_momentum == \"pullback\":\n",
        "                    param_state['cached_mom'] = torch.zeros_like(param.data)\n",
        "\n",
        "    def __getstate__(self):\n",
        "        return {\n",
        "            'state': self.state,\n",
        "            'optimizer': self.optimizer,\n",
        "            'la_alpha': self.la_alpha,\n",
        "            '_la_step': self._la_step,\n",
        "            '_total_la_steps': self._total_la_steps,\n",
        "            'pullback_momentum': self.pullback_momentum\n",
        "        }\n",
        "\n",
        "    def zero_grad(self):\n",
        "        self.optimizer.zero_grad()\n",
        "\n",
        "    def get_la_step(self):\n",
        "        return self._la_step\n",
        "\n",
        "    def state_dict(self):\n",
        "        return self.optimizer.state_dict()\n",
        "\n",
        "    def load_state_dict(self, state_dict):\n",
        "        self.optimizer.load_state_dict(state_dict)\n",
        "\n",
        "    def _backup_and_load_cache(self):\n",
        "        \"\"\"Useful for performing evaluation on the slow weights (which typically generalize better)\n",
        "        \"\"\"\n",
        "        for group in self.optimizer.param_groups:\n",
        "            for param in group['params']:\n",
        "                param_state = self.state[param]\n",
        "                param_state['backup_params'] = torch.zeros_like(param.data)\n",
        "                param_state['backup_params'].copy_(param.data)\n",
        "                param.data.copy_(param_state['cached_params'])\n",
        "\n",
        "    def _clear_and_load_backup(self):\n",
        "        for group in self.optimizer.param_groups:\n",
        "            for param in group['params']:\n",
        "                param_state = self.state[param]\n",
        "                param.data.copy_(param_state['backup_params'])\n",
        "                del param_state['backup_params']\n",
        "\n",
        "    @property\n",
        "    def param_groups(self):\n",
        "        return self.optimizer.param_groups\n",
        "\n",
        "    def step(self, closure=None):\n",
        "        \"\"\"Performs a single Lookahead optimization step.\n",
        "        Arguments:\n",
        "            closure (callable, optional): A closure that reevaluates the model\n",
        "                and returns the loss.\n",
        "        \"\"\"\n",
        "        loss = self.optimizer.step(closure)\n",
        "        self._la_step += 1\n",
        "\n",
        "        if self._la_step >= self._total_la_steps:\n",
        "            self._la_step = 0\n",
        "            # Lookahead and cache the current optimizer parameters\n",
        "            for group in self.optimizer.param_groups:\n",
        "                for param in group['params']:\n",
        "                    param_state = self.state[param]\n",
        "                    param.data.mul_(self.la_alpha).add_(param_state['cached_params'], alpha=1.0 - self.la_alpha)  # crucial line\n",
        "                    param_state['cached_params'].copy_(param.data)\n",
        "                    if self.pullback_momentum == \"pullback\":\n",
        "                        internal_momentum = self.optimizer.state[param][\"momentum_buffer\"]\n",
        "                        self.optimizer.state[param][\"momentum_buffer\"] = internal_momentum.mul_(self.la_alpha).add_(\n",
        "                            1.0 - self.la_alpha, param_state[\"cached_mom\"])\n",
        "                        param_state[\"cached_mom\"] = self.optimizer.state[param][\"momentum_buffer\"]\n",
        "                    elif self.pullback_momentum == \"reset\":\n",
        "                        self.optimizer.state[param][\"momentum_buffer\"] = torch.zeros_like(param.data)\n",
        "\n",
        "        return loss\n",
        "\n"
      ],
      "metadata": {
        "id": "dQENRG9K9bng"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model training**"
      ],
      "metadata": {
        "id": "dcjuoU2DMwDs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize adam optimizer with weight decay to minimize overfit\n",
        "\n",
        "from transformers import AdamW\n",
        "\n",
        "base  = AdamW(model.parameters(), lr=1e-4)\n",
        "optim = LookaheadWrapper(base)"
      ],
      "metadata": {
        "id": "psX_aqIbMLqf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "945e626e-f69b-4beb-d54f-33e95d3c3321"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "\n",
        "import warnings\n",
        "warnings.simplefilter(\"ignore\")\n",
        "\n",
        "\n",
        "# Initialize data loader for training data\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=16, shuffle=True)\n",
        "\n",
        "\n",
        "for epoch in range(6):\n",
        "    model.train()\n",
        "    loop = tqdm(train_loader, leave=True)\n",
        "    for batch in loop:\n",
        "        optim.zero_grad()\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        start_positions = batch['start_positions'].to(device)\n",
        "        end_positions = batch['end_positions'].to(device)\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask,\n",
        "                        start_positions=start_positions, end_positions=end_positions)\n",
        "        loss = outputs[0]\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "        loop.set_description(f'Epoch {epoch}')\n",
        "        loop.set_postfix(loss=loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7V1EoZyx-1N0",
        "outputId": "4dc9d317-01d2-441a-b5eb-dc0dab62c090"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 0: 100%|██████████| 7/7 [00:07<00:00,  1.02s/it, loss=4.26]\n",
            "Epoch 1: 100%|██████████| 7/7 [00:06<00:00,  1.13it/s, loss=3.35]\n",
            "Epoch 2: 100%|██████████| 7/7 [00:06<00:00,  1.12it/s, loss=1.46]\n",
            "Epoch 3: 100%|██████████| 7/7 [00:06<00:00,  1.13it/s, loss=0.779]\n",
            "Epoch 4: 100%|██████████| 7/7 [00:06<00:00,  1.12it/s, loss=0.131]\n",
            "Epoch 5: 100%|██████████| 7/7 [00:06<00:00,  1.11it/s, loss=0.425]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving the model in a local directory\n",
        "\n",
        "MODEL_DIR = \"./model\"\n",
        "if not os.path.exists(MODEL_DIR):\n",
        "    os.mkdir(MODEL_DIR)\n",
        "tokenizer.save_pretrained(MODEL_DIR)\n",
        "model.save_pretrained(MODEL_DIR)\n",
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
        "MODEL_DIR = \"./model\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR, use_fast=True)\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(MODEL_DIR)"
      ],
      "metadata": {
        "id": "gzxXCuc5MQ1O"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Evaluation**"
      ],
      "metadata": {
        "id": "v8CqPTidM0ku"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import f1_score\n",
        "# Switch the model to evaluation mode\n",
        "model.eval()\n",
        "model = model.to(device)\n",
        "\n",
        "validation_loader = DataLoader(valid_ds, batch_size=16)\n",
        "accuracy_scores = list()\n",
        "start_true_all, end_true_all = [], []\n",
        "start_pred_all, end_pred_all = [], []\n",
        "\n",
        "for batch in validation_loader:\n",
        "    with torch.no_grad():\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        start_true = batch['start_positions'].to(device)\n",
        "        end_true = batch['end_positions'].to(device)\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "        start_pred = torch.argmax(outputs['start_logits'], dim=1)\n",
        "        end_pred = torch.argmax(outputs['end_logits'], dim=1)\n",
        "        accuracy_scores.append(((start_pred == start_true).sum() / len(start_pred)).item())\n",
        "        accuracy_scores.append(((end_pred == end_true).sum() / len(end_pred)).item())\n",
        "        start_true_all.extend(start_true.tolist())\n",
        "        end_true_all.extend(end_true.tolist())\n",
        "        start_pred_all.extend(start_pred.tolist())\n",
        "        end_pred_all.extend(end_pred.tolist())\n",
        "\n",
        "\n",
        "# Calculate the average accuracy score\n",
        "average_accuracy = sum(accuracy_scores) / len(accuracy_scores)\n",
        "print(f\"Model's score based on Exact Match: {average_accuracy}\")\n",
        "\n",
        "\n",
        "# Calculating F1 score for start and end positions\n",
        "\n",
        "start_f1 = f1_score(start_true_all, start_pred_all, average='macro')\n",
        "end_f1 = f1_score(end_true_all, end_pred_all, average='macro')\n",
        "overall_f1 = (start_f1 + end_f1) / 2\n",
        "\n",
        "print(f\"F1 score of the model: {overall_f1:.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lrNc8_46LSYJ",
        "outputId": "586ae135-b462-4327-cdca-de5959c10b2d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model's score based on Exact Match: 0.39453125\n",
            "F1 score of the model: 0.269\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Returns answers to a given set of questions\n",
        "def get_answers_from_context(input_text: str, input_questions: list) -> list:\n",
        "    encoded_inputs = tokenizer([input_text]*len(input_questions), input_questions, truncation=True, padding=True, return_tensors=\"pt\")\n",
        "    encoded_inputs = encoded_inputs.to(device)\n",
        "    outputs = model(**encoded_inputs)\n",
        "    start_positions = torch.argmax(outputs['start_logits'], dim=1)\n",
        "    end_positions = torch.argmax(outputs['end_logits'], dim=1)  \n",
        "    answers = list()\n",
        "    for i, (start_index, end_index) in enumerate(zip(start_positions, end_positions)):\n",
        "        tokens = tokenizer.convert_ids_to_tokens(encoded_inputs['input_ids'][i][start_index:end_index+1])\n",
        "        answers.append(tokenizer.convert_tokens_to_string(tokens) )\n",
        "    print(\"Context:\")\n",
        "    print(input_text)\n",
        "    print()\n",
        "    for question, answer in zip(input_questions, answers):\n",
        "        print(f\"Q:  {question}\")\n",
        "        print(f\"A:  {answer}\")\n",
        "        print(\"-\"*60)\n",
        "    return answers\n"
      ],
      "metadata": {
        "id": "gy52P7maKukD"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sample answers from the model**"
      ],
      "metadata": {
        "id": "qM15P1eTM46m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "context = \"The modern Olympic Games or Olympics (French: Jeux olympiques)[1][2] are leading international sporting events featuring summer and winter sports competitions in which thousands of athletes from around the world participate in a variety of competitions. The Olympic Games are considered the world's foremost sports competition with more than 200 nations participating.[3] The Olympic Games are normally held every four years, alternating between the Summer and Winter Olympics every two years in the four-year period.\"\n",
        "questions = [\n",
        "    \"How often do the Olympic games hold?\",\n",
        "    \"How many nations do participate in each Olympic?\",\"what is olympics in french called as?\"\n",
        "]\n",
        "\n",
        "_ = get_answers_from_context(context, questions)"
      ],
      "metadata": {
        "id": "q19xOb-GM6XW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44576d9b-b808-4db6-deff-100fcabbc7ac"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context:\n",
            "The modern Olympic Games or Olympics (French: Jeux olympiques)[1][2] are leading international sporting events featuring summer and winter sports competitions in which thousands of athletes from around the world participate in a variety of competitions. The Olympic Games are considered the world's foremost sports competition with more than 200 nations participating.[3] The Olympic Games are normally held every four years, alternating between the Summer and Winter Olympics every two years in the four-year period.\n",
            "\n",
            "Q:  How often do the Olympic games hold?\n",
            "A:  every four years,\n",
            "------------------------------------------------------------\n",
            "Q:  How many nations do participate in each Olympic?\n",
            "A:  200\n",
            "------------------------------------------------------------\n",
            "Q:  what is olympics in french called as?\n",
            "A:  olympics\n",
            "------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context = \"Vikings is the modern name given to seafaring people primarily from Scandinavia (present-day Denmark, Norway and Sweden), who from the late 8th to the late 11th centuries raided, pirated, traded and settled throughout parts of Europe. They also voyaged as far as the Mediterranean, North Africa, the Middle East, and North America. In some of the countries they raided and settled in, this period is popularly known as the Viking Age, and the term \\\"Viking\\\" also commonly includes the inhabitants of the Scandinavian homelands as a collective whole. The Vikings had a profound impact on the Early medieval history of Scandinavia, the British Isles, France, Estonia, and Kievan Rus'.\"\n",
        "questions = [\n",
        "    \"When vikings started raided?\",\"who are vikings?\",\"Vikings had impact on which period?\"\n",
        "]\n",
        "\n",
        "_ = get_answers_from_context(context, questions)"
      ],
      "metadata": {
        "id": "ZNHN_g-MM76M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69aaba36-9eb5-4a62-c8eb-c70dd9bf0a65"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context:\n",
            "Vikings is the modern name given to seafaring people primarily from Scandinavia (present-day Denmark, Norway and Sweden), who from the late 8th to the late 11th centuries raided, pirated, traded and settled throughout parts of Europe. They also voyaged as far as the Mediterranean, North Africa, the Middle East, and North America. In some of the countries they raided and settled in, this period is popularly known as the Viking Age, and the term \"Viking\" also commonly includes the inhabitants of the Scandinavian homelands as a collective whole. The Vikings had a profound impact on the Early medieval history of Scandinavia, the British Isles, France, Estonia, and Kievan Rus'.\n",
            "\n",
            "Q:  When vikings started raided?\n",
            "A:  late 8th to the late 11th centuries raided,\n",
            "------------------------------------------------------------\n",
            "Q:  who are vikings?\n",
            "A:  vikings is the modern name given to seafaring people primarily from scandinavia (present-day denmark, norway and sweden), who from the late 8th to the late 11th centuries raided, pirated, traded and settled throughout parts of europe. they also voyaged as far as the mediterranean, north africa, the middle east, and north america. in some of the countries they raided and settled in, this period is popularly known as the viking age, and the term \"viking\" also commonly includes the inhabitants of the scandinavian homelands as a collective whole. the vikings had a profound impact on the early medieval history of scandinavia, the british isles, france, estonia, and kievan rus'.[SEP] who are vikings?[SEP]\n",
            "------------------------------------------------------------\n",
            "Q:  Vikings had impact on which period?\n",
            "A:  early medieval history of scandinavia,\n",
            "------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "B3i95evNdkDo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}